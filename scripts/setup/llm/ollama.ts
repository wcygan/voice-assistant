#!/usr/bin/env -S deno run --allow-run --allow-read --allow-write --allow-net
import $ from "https://deno.land/x/dax@0.39.2/mod.ts";

console.log("ðŸ§  Setting up Ollama LLM...");

// Check if running on macOS
const os = Deno.build.os;
if (os !== "darwin") {
  console.error("âŒ This script is designed for macOS. Please adapt for your platform.");
  Deno.exit(1);
}

try {
  // Install Ollama via Homebrew
  console.log("ðŸ“¦ Installing Ollama via Homebrew...");
  await $`brew install ollama`;
  
  // Check if Ollama service is running
  const isRunning = await $`pgrep ollama`.noThrow();
  
  if (!isRunning) {
    console.log("ðŸš€ Starting Ollama service...");
    // Start Ollama in background
    const ollamaProcess = $`ollama serve`.spawn();
    
    // Wait a moment for service to start
    await $`sleep 3`;
    
    console.log("âœ… Ollama service started");
  } else {
    console.log("âœ… Ollama service already running");
  }
  
  // Test Ollama installation
  console.log("ðŸ§ª Testing Ollama installation...");
  const version = await $`ollama --version`.text();
  console.log("Ollama version:", version);
  
  // List available models
  console.log("ðŸ“‹ Currently installed models:");
  await $`ollama list`.noThrow();
  
  // Suggest model installation
  console.log("ðŸ’¡ Recommended models to install:");
  console.log("   â€¢ ollama pull mistral        (7B general purpose)");
  console.log("   â€¢ ollama pull llama3.2       (3B efficient)");
  console.log("   â€¢ ollama pull deepseek-r1    (reasoning model - if available)");
  console.log("   â€¢ ollama pull qwen2.5        (good multilingual support)");
  
  // Check if any model is installed
  const modelList = await $`ollama list`.text().catch(() => "");
  if (!modelList.includes("NAME") || modelList.split('\n').length <= 2) {
    console.log("âš ï¸  No models installed yet. Install a model to get started:");
    console.log("   ollama pull mistral");
  }
  
  console.log("âœ… Ollama LLM setup complete!");
  console.log("ðŸ’¡ Usage:");
  console.log("   â€¢ Start service: ollama serve (in background)");
  console.log("   â€¢ Install model: ollama pull mistral");
  console.log("   â€¢ Chat: ollama run mistral");
  console.log("   â€¢ API: curl http://localhost:11434/api/generate -d '{\"model\":\"mistral\",\"prompt\":\"Hello\"}'");
  
} catch (error) {
  console.error("âŒ Ollama setup failed:", error.message);
  Deno.exit(1);
}